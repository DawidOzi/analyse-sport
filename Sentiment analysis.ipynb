{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project for Computational Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "For this project, I wanted to check the news titles from the sport.pl website regarding Polish tennis player Iga Świątek, whether they were positive, negative or perhaps neutral. The reason I wanted to check these titles is because Iga supporters believe that the titles are mostly negative in order to stir up emotions and make people want to click on the news. Iga's supporters believe that such actions are unnecessary because Iga is the second tennis player in the world (she was previously ranked number one for a long time) and she is well-known enough that it is not necessary to use such actions to make people click on these news stories. Also they think that sports portals like sport.pl should write about their subject matter reliably and not like tabloids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data\n",
    "Data for this project will be taken from the first five pages about iga swiatek on sport.pl (https://www.sport.pl/iga-swiatek, https://www.sport.pl/iga-swiatek/2, https://www.sport.pl/iga-swiatek/3, https://www.sport.pl/iga-swiatek/4, https://www.sport.pl/iga-swiatek/5). The reason for analyzing specifically such a number of pages is the issue that 200 recent titles is a suitable research sample for me. In turn, the choice of this site is due to the fact that among Iga's fans this site is considered clickbait, and sport.pl on the other hand is one of the biggest sports sites in Poland (https://media-panel.pl/pl/aktualnosci/zestawienia-tematyczne-i-funkcjonalne-grudzien-2024/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code\n",
    "## 3.1. Imports and configuration\n",
    "The code used in this project starts with the import and configuration of all the functions that will be needed in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. NLTK resources\n",
    "I then downloaded the NLTK resource detailing the punkt package, which includes a tokeniser to split the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Function: load_custom_stopwords\n",
    "I used this function to load a list of mine stopwords (i.e. words ignored in the analysis, e.g. in Polish they are: ‘i’, “ale”, “lub”), because the default stopwords list was in English and the code then didn't work so I had to generate my stopwords list in Polish (polish.stopwords.txt). The elimination of stopwords will help in a more specific analysis of titles from web pages. If the file does not exist, the function returns an empty set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            custom_stopwords = {line.strip().lower() for line in file if line.strip()}\n",
    "            return custom_stopwords\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found. Please make sure the file exists.\")\n",
    "        return set()\n",
    "STOPWORDS = load_custom_stopwords(\"polish.stopwords.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Function: get_titles_from_website\n",
    "I use this function to retrieve a web page using requests.get, also checking that the server response (HTTP status) is valid (code 200). BeautifulSoup, on the other hand, is used to analyse the HTML code. Afterwards, it searches for all elements marked as h2 and extracts text from them in the form of titles, and finally returns a list of titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_from_website(url, css_selector=\"h2\"):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to connect to the URL: {url}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    titles = [element.text.strip() for element in soup.select(css_selector)]\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Function: preprocess_titles(titles, sentiment)\n",
    "This function was used to prepare the input data for the sentiment classifier. word_tokenize splits the title into individual words, then the text is converted to lowercase (title.lower()), then removes words that are not alphanumeric (e.g. punctuation marks) and words that are in the stopwords list. Then using (dict(counter(tokens))) creates a feature dictionary that contains the number of occurrences of each word in the title. Finally, it adds the processed title and the assigned sentiment to the processed list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_titles(titles, sentiment):\n",
    "    processed = []\n",
    "    for title in titles:\n",
    "        tokens = word_tokenize(title.lower())\n",
    "        tokens = [token for token in tokens if token.isalnum() and token not in STOPWORDS]\n",
    "        processed.append((dict(Counter(tokens)), sentiment))\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Function: classify_titles(classifier, titles)\n",
    "This function was used to classify the sentiment of each article title using a trained Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_titles(classifier, titles):\n",
    "    classified = []\n",
    "    for title in titles:\n",
    "        tokens = word_tokenize(title.lower())\n",
    "        tokens = [token for token in tokens if token.isalnum() and token not in STOPWORDS]\n",
    "        features = dict(Counter(tokens))\n",
    "        sentiment = classifier.classify(features)\n",
    "        classified.append((title, sentiment))\n",
    "    return classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Function: main i.e. downloading titles, labels and results\n",
    "Firstly, titles were downloaded from 5 pages on sport.pl about Iga Świątek. Later, sample labels (‘positive’, ‘negative’, ‘neutral’) were assigned, then the Naive Bayes classifier was trained on the basis of the titles and labels. The titles were then classified using the trained model, the number of titles assigned to each category (positive, negative, neutral) was counted and, at the very end, the classification results were displayed in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    urls = [\n",
    "        \"https://www.sport.pl/iga-swiatek\",\n",
    "        \"https://www.sport.pl/iga-swiatek/2\",\n",
    "        \"https://www.sport.pl/iga-swiatek/3\",\n",
    "        \"https://www.sport.pl/iga-swiatek/4\",\n",
    "        \"https://www.sport.pl/iga-swiatek/5\"\n",
    "    ]\n",
    "    \n",
    "    all_titles = []\n",
    "    \n",
    "    print(\"Downloading article titles from given sites...\")\n",
    "    try:\n",
    "        for url in urls:\n",
    "            titles = get_titles_from_website(url)\n",
    "            all_titles.extend(titles)\n",
    "    except Exception as e:\n",
    "        print(f\"Mistake: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not all_titles:\n",
    "        print(\"No titles found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloaded {len(all_titles)} titles from {len(urls)} pages.\")\n",
    "\n",
    "    random.seed(42)\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    labeled_data = [\n",
    "        (title, random.choice(labels)) for title in all_titles\n",
    "    ]\n",
    "\n",
    "    train_data, test_data = train_test_split(labeled_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_set = []\n",
    "    for title, sentiment in train_data:\n",
    "        train_set.extend(preprocess_titles([title], sentiment))\n",
    "    \n",
    "    test_set = []\n",
    "    for title, sentiment in test_data:\n",
    "        test_set.extend(preprocess_titles([title], sentiment))\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    print(\"\\nEvaluation of the model on test data:\")\n",
    "    print(f\"Accuracy: {accuracy(classifier, test_set) * 100:.2f}%\")\n",
    "\n",
    "    classified_titles = classify_titles(classifier, all_titles)\n",
    "\n",
    "    print(\"\\nNaive Bayes classification results:\")\n",
    "    table = [[i + 1, title, sentiment] for i, (title, sentiment) in enumerate(classified_titles)]\n",
    "    headers = [\"#\", \"Title\", \"Sentiment\"]\n",
    "    print(tabulate(table, headers, tablefmt=\"grid\"))\n",
    "    \n",
    "    sentiment_counts = Counter([sentiment for _, sentiment in classified_titles])\n",
    "    print(\"\\nSummary of classification:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        print(f\"{sentiment.capitalize()}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results\n",
    "The results of the study indicated that the research problem was not confirmed because, most of the titles from sport.pl articles about Iga Świątek were neutral (70) or positive (72) and not negative (56). The exact numbers may be altered as several new articles about Iga appear throughout the day, but the difference is large enough that the research problem would still be refuted.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
